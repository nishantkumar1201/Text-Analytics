{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b094f350-a94d-4173-b89b-d7754bb1b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1-Create a Doc object from the file owlcreek.txt \n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044d0330-c621-4ff0-b9aa-3aee965a8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'owlcreek.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c278a8d1-1a1b-4e2e-888d-28a53cc0c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f9c819d-66fb-4a04-98be-11a3a5718875",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00697b04-59d6-4d97-a9d7-058ce772575d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #--- 1. Setup and Load the Text ---\n",
    "FILE_NAME = 'owlcreek.txt'\n",
    "\n",
    "# --- Load the spaCy Language Model ---\n",
    "try:\n",
    "    # Use the small model for general processing\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # print(\"spaCy model loaded successfully.\") # Commented out for cleaner output\n",
    "except OSError:\n",
    "    print(\"\\nERROR: spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run: pip install spacy && python -m spacy download en_core_web_sm\")\n",
    "    exit()\n",
    "\n",
    "# Read the full text from the file\n",
    "raw_text = \"\"\n",
    "try:\n",
    "    with open(FILE_NAME, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    # print(f\"Read {len(raw_text)} characters from '{FILE_NAME}'.\") # Commented out\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file '{FILE_NAME}': {e}\")\n",
    "    # Note: If the file is not available in the environment, this script will fail here.\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec9669c-a244-4ae2-bb87-46c2369e7f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NLP Analysis of 'An Occurrence at Owl Creek Bridge' ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Create a Doc object from the file owlcreek.txt ---\n",
    "# Passing the raw text through the nlp object processes it into a Doc object.\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "print(\"--- NLP Analysis of 'An Occurrence at Owl Creek Bridge' ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60cdda-57c0-4aac-bfec-7066d537ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Create a Doc object from the file owlcreek.txt ---\n",
    "# Passing the raw text through the nlp object processes it into a Doc object.\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "print(\"--- NLP Analysis of 'An Occurrence at Owl Creek Bridge' ---\")\n",
    "\n",
    "# --- 2. How many tokens are contained in the file? ---\n",
    "print(f\"\\n2. Total number of tokens (words/punctuation): {len(doc)}\")\n",
    "\n",
    "# --- 3. How many sentences are contained in the file? ---\n",
    "# The .sents property is a generator, so we convert it to a list to count.\n",
    "sentences = list(doc.sents)\n",
    "print(f\"3. Total number of sentences: {len(sentences)}\")\n",
    "\n",
    "# --- 4. Print the second sentence in the document ---\n",
    "if len(sentences) > 1:\n",
    "    second_sentence = sentences[1]\n",
    "    print(f\"\\n4. The second sentence is:\\n'{second_sentence}'\")\n",
    "else:\n",
    "    print(\"\\n4. Error: Document contains less than two sentences.\")\n",
    "    second_sentence = None\n",
    "\n",
    "\n",
    "# --- 5. For each token in the sentence above, print its text, POS tag, dep tag and lemma. ---\n",
    "if second_sentence:\n",
    "    print(\"\\n5. Token Details for the Second Sentence:\")\n",
    "    print(\"{:<15} {:<10} {:<10} {:<15}\".format(\"TEXT\", \"POS\", \"DEP\", \"LEMMA\"))\n",
    "    print(\"-\" * 50)\n",
    "    for token in second_sentence:\n",
    "        # Format the output clearly\n",
    "        print(\"{:<15} {:<10} {:<10} {:<15}\".format(\n",
    "            token.text,      # The token text\n",
    "            token.pos_,      # Part-of-Speech tag (e.g., NOUN, VERB)\n",
    "            token.dep_,      # Syntactic dependency tag (e.g., nsubj, ROOT)\n",
    "            token.lemma_     # Base form of the word\n",
    "        ))\n",
    "\n",
    "\n",
    "# --- 6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text. ---\n",
    "# --- 7. Print the text surrounding each found match. ---\n",
    "print(\"\\n6. & 7. Finding and Printing Matches for 'swimming vigorously':\")\n",
    "\n",
    "# Initialize the Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern for \"swimming vigorously\"\n",
    "# We match the literal text of both words.\n",
    "pattern = [{\"LOWER\": \"swimming\"}, {\"LOWER\": \"vigorously\"}]\n",
    "\n",
    "# Add the pattern to the matcher under the ID 'SWIMMING'\n",
    "matcher.add(\"SWIMMING\", [pattern])\n",
    "\n",
    "# Find matches in the Doc object\n",
    "matches = matcher(doc)\n",
    "\n",
    "if matches:\n",
    "    print(f\"Found {len(matches)} match(es) for 'swimming vigorously'.\")\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the match span\n",
    "        span = doc[start:end]\n",
    "        \n",
    "        # Get the surrounding context: 10 tokens before and 10 tokens after the match.\n",
    "        # Ensure we don't go below index 0 or above the document length.\n",
    "        context_start = max(0, start - 10)\n",
    "        context_end = min(len(doc), end + 10)\n",
    "        \n",
    "        context_span = doc[context_start:context_end]\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Match found: '{span.text}'\")\n",
    "        print(f\"Surrounding Text:\\n'{context_span.text}'\")\n",
    "else:\n",
    "    print(\"No matches found for 'swimming vigorously'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ae3ac-4c93-4eb3-bdb0-c82507d77687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
